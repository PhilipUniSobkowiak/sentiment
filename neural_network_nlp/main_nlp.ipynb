{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c68264d-78cd-48aa-9558-de6307dbcded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "# import matplotlib as mpl \n",
    "# import matplotlib.cm as cm \n",
    "# import matplotlib.pyplot as plt \n",
    "# import plotly.graph_objects as go\n",
    "# import seaborn as sns\n",
    "import re \n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7af2070-b8b0-488f-a308-a5514d60fe4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28619"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Json file lesen into list of dict\n",
    "with open('data/Sarcasm_Headlines_Dataset.json', 'r') as sarcasm_headlines_file:\n",
    "    dictionary_data = []\n",
    "    \n",
    "    # Read each line of the file\n",
    "    for line in sarcasm_headlines_file:\n",
    "        dictionary_data.append(json.loads(line))\n",
    "\n",
    "\n",
    "#I will use this dictionary_data to create the np arrays and token dictionary\n",
    "len(dictionary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb55023-9258-4f0a-9e1b-79141809b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pondas to check for \n",
    "#(1) Non zero values in rows \n",
    "#(2) Are the labels only 0 and 1 \n",
    "#(3) Make sure strings are not empty or only have a whitespace\n",
    "#-------------------\n",
    "#Class might be overkill. Oh well\n",
    "class dataReader():\n",
    "    def __init__(self,file_name):\n",
    "        self.file = file_name\n",
    "        self.data_frame = None\n",
    "        \n",
    "    #Parse the file into a dataframe\n",
    "    #-------------------\n",
    "    def create_df(self):\n",
    "        json_objects = []\n",
    "        with open(self.file, 'r') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    json_object = json.loads(line)\n",
    "                    json_objects.append(json_object)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON on line: {line.strip()}\")\n",
    "                    print(f\"Error message: {str(e)}\")\n",
    "        self.data_frame = pd.DataFrame(json_objects)\n",
    "        return self.data_frame\n",
    "\n",
    "    # (1) Non zero values in rows\n",
    "    #-------------------\n",
    "    def remove_null_rows(self):\n",
    "        size_before_removal = len(self.data_frame)\n",
    "        self.data_frame = self.data_frame.dropna() \n",
    "        return f\"Amount of rows of data_frame before removal of null rows: {size_before_removal} VS after removal : {len(self.data_frame)}\"\n",
    "\n",
    "    # (2) Check for the set of labels\n",
    "    #-------------------\n",
    "    def set_of_labels(self,column_name):\n",
    "        label = self.data_frame[column_name]\n",
    "        set_of_labels = label.unique()\n",
    "        return f\"Only the following labels exist : {set_of_labels}\"\n",
    "\n",
    "    # (3) No empty strings AND no whitespaces\n",
    "    # My reasoning is that dropna() is checking for null which is  != ' ' and ''\n",
    "    #-------------------\n",
    "    def check_for_empty_strings(self,column_name):\n",
    "        bool_non_empty_strings = ((df[column_name] != '').all and (df[column_name] != ' ').all())\n",
    "        if bool_non_empty_strings == True:\n",
    "            return f\"No empty Strings in this column, nor are there any whitespaces\"\n",
    "        else:\n",
    "            return f\"yep Empty strings, better implement a solution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f198875-c2ae-4291-8835-778ab2cf58e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = dataReader('data/Sarcasm_Headlines_Dataset.json')\n",
    "df = obj.create_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a1e765-5bd0-40e1-aab6-00f8aa33f176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amount of rows of data_frame before removal of null rows: 28619 VS after removal : 28619'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.remove_null_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8311f4-c241-4470-bbee-5ac212801774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Only the following labels exist : [1 0]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.set_of_labels(\"is_sarcastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36cb0ec5-7623-4aa2-8837-35bc7bb4fbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No empty Strings in this column, nor are there any whitespaces'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.check_for_empty_strings(\"headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10abb927-dc22-45b7-9523-b81c069f3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of rows hasn't changed. Thus, I will be using the list created from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f700394f-6294-4a4e-ad36-52510350d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of words\n",
    "dict_tokens = {}\n",
    "train_size = int(0.2 * len(dictionary_data))\n",
    "def getXY(data,size,dict_tokens):\n",
    "    y_train = np.array([dictionary[\"is_sarcastic\"] for dictionary in dictionary_data[:train_size] if dictionary[\"is_sarcastic\"] in (0, 1)])\n",
    "\n",
    "\n",
    "    max_Sequence_Length =0 \n",
    "    word_pattern = r'\\b\\w+\\b'  # Match whole words only\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    contraction_pattern = r\"\\b(?:[a-zA-Z]+(?:'[a-zA-Z]+)?)-?(?:[a-zA-Z]+(?:'[a-zA-Z]+)?)?\\b\"\n",
    "    x_as_list = []\n",
    "    combined_pattern = f'{contraction_pattern}|{word_pattern}|{punctuation_pattern}'\n",
    "    for dictionary in dictionary_data[:train_size]:\n",
    "        x = re.findall(combined_pattern, dictionary['headline'])\n",
    "        if max_Sequence_Length < len(x):\n",
    "            max_Sequence_Length = len(x)\n",
    "        sequence_as_numbers = []\n",
    "        for token in x:\n",
    "            lowercase_token = token.lower()\n",
    "            if token.lower() not in dict_tokens.keys():\n",
    "                idx = (len(dict_tokens)) + 1\n",
    "                dict_tokens[lowercase_token] = (len(dict_tokens)) + 1\n",
    "                sequence_as_numbers.append(idx)\n",
    "            else:\n",
    "                sequence_as_numbers.append(dict_tokens[token])\n",
    "        x_as_list.append(sequence_as_numbers)\n",
    "    amountDataPoints = len(x_as_list)\n",
    "    return x_as_list,y_train, amountDataPoints, max_Sequence_Length, dict_tokens\n",
    "    \n",
    "x_as_list , y, amountDataPoints, max_Sequence_Length,tokens= getXY(dictionary_data,train_size,dict_tokens)\n",
    "\n",
    "#Padding to array\n",
    "def add_Padding(data,amountDP,max_Sequence_Length):\n",
    "    zeros_shape = (amountDP,max_Sequence_Length)\n",
    "    np_input = np.zeros(zeros_shape)\n",
    "    for idx , value in enumerate(x_as_list):  \n",
    "        length_sequence = len(value)   \n",
    "        sequence = np.array(value)\n",
    "        \n",
    "        #start_index = (max_Sequence_Length - length_sequence) \n",
    "        # I keep padding at the end for now \n",
    "        #---------------------------------------\n",
    "        np_input[idx, :length_sequence] = sequence\n",
    "    return np_input\n",
    "X_padding = add_Padding(x_as_list,amountDataPoints,max_Sequence_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03ee8d11-b9fc-4b60-bdb4-d8170568d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I forgot when i used this. I am just keeping it for now \n",
    "#---------------------------------------\n",
    "# def reshaper(x):\n",
    "#     # Get the dimensions of the input array\n",
    "#     batch_size, seq_length = x.shape\n",
    "    \n",
    "#     d_model = 512\n",
    "    \n",
    "#     # Initialize a 3D array with zeros\n",
    "#     batch = np.zeros((batch_size, 1, seq_length))\n",
    "    \n",
    "#     # Iterate over each row in the input array\n",
    "#     for idx, row in enumerate(x):\n",
    "#         print(row.tolist())\n",
    "\n",
    "#         break\n",
    "#         # Copy the row into the corresponding slice of the 3D array\n",
    "#         batch[idx, 0, :] = row\n",
    "        \n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77797601-1f46-4ca0-87e4-31a700028181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar categories for words get similar 2D space with the embedding Matrix\n",
    "#Vector for each existing token. randomized initial values\n",
    "class begin_encoder():\n",
    "    def __init__(self,dict_tokens, X_padding,max_sequence,d_model):\n",
    "\n",
    "        self.max_sequence_length = max_sequence\n",
    "        self.x_padding = X_padding\n",
    "        self.dict = dict_tokens\n",
    "        self.output = None\n",
    "        self.embedding_matrix = None\n",
    "        self.positional_encoding = None\n",
    "        self.d_model = d_model\n",
    "        self.batch_size = self.x_padding.shape[0]\n",
    "        self.batch = None\n",
    "        self.mask = None\n",
    "        \n",
    "    #Each dictionary token gets an embedding vector\n",
    "    def create_each_embedding_vector(self):\n",
    "        amountOfTokens = len(self.dict)\n",
    "        self.embedding_matrix = np.random.randn(amountOfTokens,self.d_model)\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    # X_padding reshape (DataPoints, Max sentence length) -> (Batch / Data, Words, embedded vector for word)\n",
    "    def embed_sentences(self):  # Renamed the method to avoid conflict\n",
    "        self.batch= np.zeros((self.batch_size, self.max_sequence_length, self.d_model))  # Changed 512 to self.d_model\n",
    "        for idx, row in enumerate(self.x_padding):\n",
    "            for pos, word_idx in enumerate(row):\n",
    "                inedces = int(word_idx)\n",
    "                if inedces == 0:\n",
    "                    self.batch[idx,pos,:] = 0\n",
    "                    break\n",
    "                else:\n",
    "                    self.batch[idx, pos, :] = self.embedding_matrix[inedces-1,:]\n",
    "        return self.batch\n",
    "\n",
    "    def mask_for_attention(self):\n",
    "        self.mask = self.batch != 0\n",
    "        return self.mask\n",
    "    #I am not sure if the padded area wont change the outcome? since the sentences are at the end of padding and the sin and cos doesnt take that int account\n",
    "    \n",
    "    def positional_Encoding(self):\n",
    "        col_em = self.embedding_matrix.shape[1]\n",
    "        self.positional_encoding = np.zeros((self.max_sequence_length,col_em))\n",
    "        position = np.arange(self.max_sequence_length).astype(np.float32).reshape(-1, 1)\n",
    "        divisor = np.exp(np.arange(0, self.d_model, 2).astype(np.float32) * (-np.log(10000.0) / self.d_model))\n",
    "        self.positional_encoding[:,0::2] = np.sin(position* divisor)\n",
    "        self.positional_encoding[:, 1::2] = np.cos(position* divisor)\n",
    "        return self.positional_encoding\n",
    "        \n",
    "    def encoding(self):\n",
    "        self.output = np.zeros_like(self.batch, dtype=float) \n",
    "        for idx,dataPoint in enumerate(self.batch):\n",
    "            self.output[idx] = dataPoint + self.positional_encoding\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2983f-47e6-4d6c-8a40-574b40482c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37fd9b90-07cc-441b-a384-1d948f6a9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after pos encoding and after multihead attention\n",
    "class Layer_normalization():\n",
    "    def __init__(self):\n",
    "        self.eps = 10**-6\n",
    "        self.weight = np.ones(1)\n",
    "        self.bias = np.zeros(1)\n",
    "    def forward(self,sequence):\n",
    "        mean = np.mean(sequence,axis=1, keepdims=True)\n",
    "        std = np.std(sequence,axis=1,keepdims=True)\n",
    "        return (self.weight * (sequence - mean) / np.sqrt( (std**2+ self.eps))) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ff5aa0-48e2-4f96-b80c-f89022190fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout i guess was so it randomly set avlues to 0 and reduces overfitting\n",
    "#after normalization\n",
    "class FeedForwardBlock:\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        self.W1 = np.random.randn(d_model, d_ff)  # Weight matrix for the first linear transformation\n",
    "        self.b1 = np.zeros(d_ff)  # Bias vector for the first linear transformation\n",
    "        \n",
    "        self.W2 = np.random.randn(d_ff, d_model)  # Weight matrix for the second linear transformation\n",
    "        self.b2 = np.zeros(d_model)  # Bias vector for the second linear transformation\n",
    "        self.dropout = dropout\n",
    "        self.h1 = None\n",
    "        self.h1_after_relu = None\n",
    "        self.h2 = None\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_ff)\n",
    "        h1 = np.matmul(x, self.W1) + self.b1\n",
    "        self.h1 = h1\n",
    "        h1_relu = np.maximum(h1, 0)  # ReLU activation for non linear function\n",
    "        self.h1_after_relu = h1_relu\n",
    "        # Apply dropout\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.rand(*h1.shape) < self.dropout\n",
    "            h1 = np.where(mask, 0, h1)\n",
    "    \n",
    "        # (batch_size, seq_len, d_ff) --> (batch_size, seq_len, d_model)\n",
    "        h2 = np.matmul(h1, self.W2) + self.b2\n",
    "        self.h2 = h2\n",
    "        return h2\n",
    "\n",
    "    def backward(self, grad_loss, learning_rate):\n",
    "        # Backpropagation\n",
    "        pass\n",
    "\n",
    "        return grad_h1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f58849c-8077-4b89-b235-484643d19720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask e softmax to hide negative values will go close to 0\n",
    "class MultiHeadAttentionBlock():\n",
    "    def __init__(self,d_model,h,dropout):\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = dropout\n",
    "        assert d_model % h == 0 #make sure i can actually split the model into amount heads\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = np.random.randn(d_model, d_model)\n",
    "        self.w_k = np.random.randn(d_model, d_model)\n",
    "        self.w_v = np.random.randn(d_model, d_model)\n",
    "\n",
    "        self.w_o = np.random.randn(d_model, d_model)\n",
    "        self.dropout = dropout\n",
    "        self.MH_a = None\n",
    "        self.weighted_sum = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, dropout):\n",
    "        #d_k = len of col for each head\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention_scores = np.matmul(query, key.transpose((0, 1, 3, 2))) / np.sqrt(query.shape[-1])\n",
    "    \n",
    "        # Apply softmax\n",
    "        attention_scores = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n",
    "        attention_scores /= np.sum(attention_scores, axis=-1, keepdims=True)\n",
    "       \n",
    "        # Apply dropout\n",
    "        if dropout > 0:\n",
    "            mask = np.random.rand(*attention_scores.shape) < dropout\n",
    "            attention_scores_applied_dropout = np.where(mask, 0, attention_scores)\n",
    "    \n",
    "        # Calculate weighted sum of values\n",
    "        weighted_sum = np.matmul(attention_scores_applied_dropout, value)\n",
    "    \n",
    "        # Return weighted sum and attention scores\n",
    "        attention_scores_applied_dropout = attention_scores_applied_dropout\n",
    "        return weighted_sum, attention_scores_applied_dropout\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        q_snake = np.matmul(q , self.w_q)  # Apply linear transformation\n",
    "        k_snake = np.matmul(k, self.w_k)  # Apply linear transformation\n",
    "        v_snake = np.matmul(v, self.w_v)  # Apply linear transformation\n",
    "        batch_size, seq_len, _ = q_snake.shape\n",
    "\n",
    "        # Reshape query, key, and value for multi-head attention\n",
    "        if mask is not None:\n",
    "            masked_q = np.where(mask == False, -1e9, q_snake)\n",
    "            masked_k= np.where(mask == False, -1e9, k_snake)\n",
    "            masked_v = np.where(mask == False, -1e9,v_snake)\n",
    "        #  mask_reshaped = np.reshape(mask, (batch_size, seq_len, self.h, self.d_k))\n",
    "\n",
    "        q_snake = np.reshape(q_snake, (batch_size,seq_len, self.h, self.d_k))\n",
    "        k_snake= np.reshape(k_snake, (batch_size, seq_len, self.h, self.d_k))\n",
    "        v_snake = np.reshape(v_snake, (batch_size,seq_len, self.h, self.d_k))\n",
    "        # Transpose query, key, and value for multi-head attention\n",
    "        # mask_transposed = np.transpose(mask_reshaped,(0,2,1,3))       \n",
    "        query_transposed = np.transpose(q_snake, (0, 2, 1, 3))  # (batch_size, h, seq_len, d_k)\n",
    "        key_transposed = np.transpose(k_snake, (0,2, 1, 3))  # (batch_size, h, seq_len, d_k)\n",
    "        value_transposed = np.transpose(v_snake, (0, 2, 1, 3))  # (batch_size, h, seq_len, d_k)\n",
    "        \n",
    "        # Calculate attention\n",
    "        x, attention_scores = MultiHeadAttentionBlock.attention(query_transposed, key_transposed, value_transposed, self.dropout)\n",
    "        # Combine all the heads together\n",
    "        x = np.transpose(x, (0, 2, 1, 3))  # (batch_size, seq_len, h, d_k)\n",
    "        x = np.reshape(x, (batch_size, seq_len, -1))  # (batch_size, seq_len, h * d_k)` \n",
    "        self.MH_a = np.matmul(x,self.w_o)\n",
    "        return self.MH_a, attention_scores\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        pass\n",
    "\n",
    "\n",
    "        return grad_input_q, grad_input_k, grad_input_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b97437-29ae-461a-8706-8dbb01a26779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection from norm and mutlihead to Feedforward and to -> norm that is after feedforward \n",
    "class ResidualConnection:\n",
    "    def __init__(self, dropout):\n",
    "        self.dropout= dropout\n",
    "        self.norm = Layer_normalization()  # Assuming LayerNormalization is defined elsewhere\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # Apply dropout manually\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.rand(*x.shape) < self.dropout\n",
    "            x_drop = np.where(mask, 0, x)\n",
    "        else:\n",
    "            x_drop = x\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_norm = self.norm.forward(x_drop)\n",
    "        \n",
    "        # Apply sublayer transformation\n",
    "        x_sub = sublayer(x_norm)\n",
    "\n",
    "        # Add residual connection\n",
    "        return x + x_sub\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf058cc4-827e-4342-b347-658989bcae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    def __init__(self, dict_tokens, max_sequence, d_models , num_heads, d_ff, dropout):\n",
    "        self.x_padded = None\n",
    "        self.dict_tokens = dict_tokens\n",
    "        self.max_sequence = max_sequence\n",
    "        self.d_model = d_models\n",
    "        self.self_attention = None\n",
    "        self.feed_forward = None\n",
    "        self.residual_connection_after_mha = None\n",
    "        self.residual_connection_after_ff= None\n",
    "        self.heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.d_ff = d_ff\n",
    "        self.output = None\n",
    "        self.embedding_matrix_grads = None\n",
    "        self.embedding_matrix = None\n",
    "        self.attention_scores = None\n",
    "        \n",
    "    def forward(self, X_padding):\n",
    "        self.x_padded = X_padding\n",
    "       \n",
    "        #===================================\n",
    "        #begin_encoding_ positional + embedding\n",
    "        #Here i get the embedding vector for the words\n",
    "        encoder = begin_encoder(self.dict_tokens,self.x_padded,self.max_sequence,self.d_model)\n",
    "        encoder.create_each_embedding_vector()\n",
    "        x_embeddings = encoder.embed_sentences()\n",
    "        self.embedding_matrix = x_embeddings\n",
    "        # Shape : (32, 51, 512) (batch, sentence length, embedding vector)\n",
    "        #===================================\n",
    "        \n",
    "        mask = encoder.mask_for_attention()\n",
    "        encoder.positional_Encoding()\n",
    "        positional_encoded_input = encoder.encoding()\n",
    "        axes = self.x_padded.shape[0]\n",
    "        #On all the 32 datapoints\n",
    "        mask_batched = mask[0:axes,:,:]\n",
    "        print(mask_batched.shape)\n",
    "        \n",
    "        #===================================\n",
    "        #MHA\n",
    "        self.self_attention = MultiHeadAttentionBlock(self.d_model,self.heads,self.dropout)\n",
    "        mha, self.attention_scores = self.self_attention.forward(x_embeddings, x_embeddings, x_embeddings, mask_batched)\n",
    "        self.residual_connection_after_mha = ResidualConnection(self.dropout)\n",
    "        multi_head_normalized = self.residual_connection_after_mha.forward(x_embeddings, lambda x: self.self_attention.forward(x_embeddings, x_embeddings, x_embeddings, mask_batched)[0])\n",
    "        \n",
    "\n",
    "        #======================================\n",
    "        #feedforward\n",
    "        self.feed_forward = FeedForwardBlock(self.d_model,self.d_ff,self.dropout)\n",
    "        self.residual_connection_after_ff= ResidualConnection(self.dropout)\n",
    "        self.output = self.residual_connection_after_ff.forward(multi_head_normalized, lambda x:self.feed_forward.forward(multi_head_normalized))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, loss_from_classifier,learning_rate):\n",
    "        pass\n",
    "\n",
    "        print(q,k,v)\n",
    "    def update_embedding_vectors(self, learning_rate):\n",
    "        # Update embedding vectors using gradient descent\n",
    "        self.embedding_matrix -= learning_rate * self.embedding_matrix_grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c602ce78-ac13-4234-b7d9-20896bca9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SoftmaxActivation:\n",
    "#     def __init__ (self):\n",
    "#         self.output = None\n",
    "#     def forward(self,input):\n",
    "#         #exp is super high we subtract the max value so its only 0 or negative values which on the exp curve can only go up to 1\n",
    "#         exp = np.exp(input - np.max(input,axis =1 , keepdims = True) )\n",
    "#         sum = np.sum(exp,axis =1 , keepdims =True)\n",
    "#         self.output = exp / sum\n",
    "\n",
    "#         return self.output\n",
    "#     def backward(self, grad):\n",
    "#         J = np.diag(self.output.flatten()) - np.dot(self.output.T, self.output)\n",
    "#         # Compute gradient of the loss with respect to the input of softmax\n",
    "#         grad_input = np.dot(grad, J)\n",
    "#         return grad_input\n",
    "\n",
    "\n",
    "class Layerdense:\n",
    "    def __init__(self,n_inputs,n_nodes,amount_data):\n",
    "        self.amount_data = amount_data\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_nodes\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs,self.n_neurons)\n",
    "        self.bias = np.random.randn(1,self.n_neurons)\n",
    "        self.inputs = None\n",
    "\n",
    "        self.derivative_inputs = None\n",
    "        self.derivative_bias = None\n",
    "        self.derivative_weights = None\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        #input shape = batchsize x sequence max length x embedding vector length d_model\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(self.inputs,self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        # self.inputs.shape (32,5) \n",
    "        # self.weights (5, 2) weights\n",
    "        # derivative respect to w = X * derivative of Loss\n",
    "        # grad_ouput shape = (32,2)\n",
    "        #==================================================\n",
    "        self.derivative_weights = np.dot(self.inputs.T,grad_output)\n",
    "        \n",
    "        # Why did chatgpt tell me derivative of bias is usually sum over grad_output?)\n",
    "        # derivative bias = w x + b => 1 * loss of the successor layer/loss \n",
    "        #=========================\n",
    "        self.derivative_bias =  grad_output.mean(axis=0)\n",
    "        # Shape weights = (5, 2) \n",
    "        self.derivative_inputs = np.dot(grad_output,self.weights.T)\n",
    "\n",
    "        # UPDATING the weights and biases\n",
    "        # self.weights (5, 2) weights\n",
    "        #===============================\n",
    "        self.weights = self.weights - learning_rate * self.derivative_weights\n",
    "        self.bias = self.bias - learning_rate * self.derivative_bias\n",
    "        \n",
    "        # grad_input = np.dot(grad_output, self.weights.T)\n",
    "        # grad_weights = np.dot(self.inputs.T, grad_output)\n",
    "        # self.weights -= learning_rate * grad_weights\n",
    "        # self.bias -= learning_rate * grad_output.mean(axis=0)\n",
    "        return self.derivative_inputs\n",
    "\n",
    "\n",
    "class ActivationSigmoid:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the log-sum-exp trick\n",
    "        maximum = x.max()\n",
    "        x -= maximum\n",
    "        self.output = 1 / (1 + np.exp((x)))\n",
    "        return self.output\n",
    "    def backward(self,x , grad):\n",
    "        # sigmoid * (1- sigmoid)\n",
    "        derivative = grad * 1 / (1 + np.exp((x))) * ( 1 - (1 / (1 + np.exp((x)))))\n",
    "        return derivative\n",
    "\n",
    "class Classification:\n",
    "    def __init__(self, input_size, hidden_size, output_size, encoded_inputs):\n",
    "        self.batch_size = encoded_inputs.shape[0]\n",
    "        self.sequence_length = encoded_inputs.shape[1]\n",
    "        self.input = encoded_inputs\n",
    "        \n",
    "        self.layer1 = Layerdense(input_size, hidden_size, self.batch_size)\n",
    "        self.activation1 = ActivationSigmoid()\n",
    "        self.layer2 = Layerdense(hidden_size, output_size, self.batch_size)\n",
    "        self.activation2 = ActivationSigmoid()\n",
    "\n",
    "    def compute_loss(self,target):\n",
    "        predictions = self.activation2.output\n",
    "        cross_entropy_loss = -np.mean(target * np.log(predictions + 1e-9) + (1 - target) * np.log(1 - predictions + 1e-9))\n",
    "        return cross_entropy_loss\n",
    "        \n",
    "    def forward(self):\n",
    "        self.layer1.forward(self.input)\n",
    "        self.activation1.forward(self.layer1.output)\n",
    "        self.layer2.forward(self.activation1.output)\n",
    "        #binary classification use sigmoid\n",
    "        self.activation2.forward(self.layer2.output)\n",
    "        return self.activation2.output\n",
    "\n",
    "    def backward(self, target, learning_rate):\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-7  \n",
    "        assert self.activation2.output.shape[0] != 0, f'cant div by {self.activation2.output.shape[0]}'\n",
    "        derivative_cross_entropy = ((self.activation2.output - target) / (self.activation2.output + epsilon)  * ( 1- self.activation2.output + epsilon)) / self.activation2.output.shape[0]\n",
    "        #derivative Cross entropy shape (32,2) \n",
    "        gradients_layer1 = self.layer2.backward(derivative_cross_entropy, learning_rate)\n",
    "        \n",
    "        gradients_activation1 = self.activation1.backward(self.layer1.output,gradients_layer1)\n",
    "        grad_layer = self.layer1.backward(gradients_activation1, learning_rate)\n",
    "        return grad_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "55b7aa01-8d33-4ada-8c50-c5a8d283dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_encoder_loss(encoder_output, y, predictions):\n",
    "    # Predict using the classifier\n",
    "\n",
    "    cross_entropy_loss = -np.mean(y * np.log(predictions + 1e-9) + (1 - y) * np.log(1 - predictions + 1e-9))\n",
    "\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "99826f09-928c-4322-aaf7-a858c34d1393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 51, 512)\n",
      "(32, 512) encoded\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "10.361632917973203 here ()\n",
      "(32, 512) gradloss Shapem (32, 51, 1000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 51 is different from 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m gradient \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mbackward(y_batch,learning_rate_classifier)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Compute loss for encoder\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m grad_encoder \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mbackward(gradient,learning_rate_classifier)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Forward pass for classifier\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#2nd neuron\u001b[39;00m\n\u001b[0;32m     53\u001b[0m accuracy2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(pred[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m y_batch)\n",
      "Cell \u001b[1;32mIn[19], line 56\u001b[0m, in \u001b[0;36mEncoderBlock.backward\u001b[1;34m(self, loss_from_classifier, learning_rate)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss_from_classifier,learning_rate):\n\u001b[1;32m---> 56\u001b[0m     gradient_feed_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward\u001b[38;5;241m.\u001b[39mbackward(loss_from_classifier, learning_rate)\n\u001b[0;32m     57\u001b[0m     q, k,v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mbackward(gradient_feed_forward,learning_rate)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(q,k,v)\n",
      "Cell \u001b[1;32mIn[16], line 38\u001b[0m, in \u001b[0;36mFeedForwardBlock.backward\u001b[1;34m(self, grad_loss, learning_rate)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Gradient of loss w.r.t. the ReLU activation\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(grad_loss\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradloss Shapem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh1_after_relu\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 38\u001b[0m grad_h1_after_relu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(grad_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh1_after_relu)\n\u001b[0;32m     39\u001b[0m grad_h1 \u001b[38;5;241m=\u001b[39m grad_h1_after_relu \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh1 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Gradient of loss w.r.t. the first linear transformation parameters\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 51 is different from 512)"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "batch_size = 32  \n",
    "learning_rate_encoder = 0.0001  \n",
    "learning_rate_classifier = 0.001  \n",
    "num_epochs = 10  \n",
    "d_model = 512\n",
    "num_heads = 4\n",
    "d_ff = 1000\n",
    "dropout = 0.1\n",
    "# Instantiate EncoderBlock\n",
    "encoder = EncoderBlock(tokens, max_Sequence_Length, d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "accuracy1_list = []\n",
    "accuracy2_list = []\n",
    "# Loop over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    indices = np.arange(len(X_padding))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train_shuffled = X_padding[indices]\n",
    "    y_train_shuffled = y[indices]\n",
    "    \n",
    "    # Loop over mini-batches\n",
    "    for i in range(0, 64, batch_size):\n",
    "        # Extract mini-batch\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Forward pass for encoder BLock transformer\n",
    "        encoder_output = encoder.forward(X_batch)\n",
    "        encoder_output_pooled = np.mean(encoder_output,axis=1)\n",
    "        print(encoder_output_pooled.shape ,'encoded')\n",
    "        classifier = Classification(d_model, 5, 2, encoder_output_pooled)\n",
    "        \n",
    "        pred = classifier.forward()\n",
    "        pred[pred >0.6] = 1\n",
    "        pred[pred <0.6] = 0\n",
    "\n",
    "        y_batch = y_batch.reshape((-1,1))\n",
    "        \n",
    "        loss = classifier.compute_loss(y_batch)\n",
    "        \n",
    "        gradient = classifier.backward(y_batch,learning_rate_classifier)\n",
    "\n",
    "        # Compute loss for encoder\n",
    "        grad_encoder = encoder.backward(gradient,learning_rate_classifier)\n",
    "    \n",
    "        # Forward pass for classifier\n",
    "\n",
    "\n",
    "        #2nd neuron\n",
    "        accuracy2 = np.mean(pred[:,1] == y_batch)\n",
    "        accuracy1 = np.mean(pred[:,0] == y_batch)\n",
    "        accuracy1_list.append(accuracy1)\n",
    "        accuracy2_list.append(accuracy2)\n",
    "plt.plot(range(num_epochs), accuracy1_list[:num_epochs], label='Neuron 1 Accuracy')\n",
    "plt.plot(range(num_epochs), accuracy2_list[:num_epochs], label='Neuron 2 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19700bfb-757f-4df7-80ec-8b59079bc2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a534a1-ef9e-4265-8b62-a899e2f85f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49caeaeb-a1b9-4d1d-9f55-69ddd8159fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0b867-f68f-4459-aeee-30736abc4903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7b4c8-f68f-41cd-a7e9-accba4fd4a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4748d-ae98-4bc0-b48f-2ee50560b811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1dc36-c6a7-4d54-94d1-01e58fe8d4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
